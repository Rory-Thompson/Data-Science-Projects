---
title: "Retail Project"
author: "Rory Thompson_rtho0020"
date: '2022-05-16'
output: html_document
---

```{r setup, include=FALSE}
library(fpp3)

# Use your student ID as the seed


```


## Statistical features of the data.
We First will run through some basic features from the original data before we set the seed to my own data.
```{r}
aus_retail %>% features(Turnover, quantile) %>% arrange(desc(`50%`)) %>%
  head(5)



```
First we will look at some basic feature analysis. We can see from this that new south wales food retailing has the highest amount of median sales, followed by the new south wales supermarket and grocery stores.



```{r}

aus_retail %>%
  features(Turnover, feat_stl) %>%
  ggplot(aes(x = trend_strength, y = seasonal_strength_year,
             col = Industry)) +
  geom_point() +
  facet_wrap(vars(State))
```
The above graphic shows that for all of the different states and industries there is generally a strong trend.There is a number of different industries so it is a little hard to tell which industries are showing the greatest seasonal trends, but it would appear to be the other retailing services.
```{r}
aus_retail %>% features(Turnover, feat_spectral) %>%
  arrange(spectral_entropy) %>%
  head(5)




```
Another statistic we can attempt to select is the spectral entropy, a measure of how easy a series is to forecast. Here we will just have a look at the lowest spectral entropy value. We can see hear that Queensland other retailing n. e c. has a spectral entropy closest to 0, hence it should be the easiest to forecast. This suggests that it has a high amount seasonality as well as a strong trend.



```{r}

set.seed(31479839)
myseries <- aus_retail %>%
  # Remove discontinued series
  filter(!(`Series ID` %in% c("A3349561R","A3349883F","A3349499L","A3349902A",
                        "A3349588R","A3349763L","A3349372C","A3349450X",
                        "A3349679W","A3349378T","A3349767W","A3349451A"))) %>%
  # Select a series at random
  filter(`Series ID` == sample(`Series ID`,1))

myseries %>% autoplot(Turnover) + labs(y = "Turnover ($m)", title = "My data")
```




## Transformation and Differencing
In general there could be a few issues with this data that may require a transformation. One could be the impact of inflation, the next could be the impact of population increases. based on the data that we have this would be hard to take into account for.

One aspect of the graphic that we do notice is that the variation between seasons increases with the level of the series. One thing we can do is a box cox transformation. Below we will perform a box cox transformation to the data.
```{r}


lambda <- myseries %>%
  features(Turnover, features = guerrero) %>%
  pull(lambda_guerrero)


myseries %>%
  autoplot(box_cox(Turnover, lambda)) + labs(y = "Turnover ($m)", title = "Box cox Transformation")



```





We can see from the above plot that the peaks and troughs have now become somewhat more even from the box cox transformation, this will make forecasting easier and hopefully more accurate in the future.

From this the graph is still obviously none stationary, to make this graph stationary what we want to do is peform differencing.

```{r}
myseries %>%
  mutate(diff_Turnover= difference(difference(box_cox(Turnover, lambda),12),1)) %>%
  autoplot(diff_Turnover)  + labs(y = "Diffed Turnover $ m", title = "Diffed data")




```

From above the graph does appear to lose the strong trend. Further because the data is monthly we must attempt to remove the seasonality in the data, hence we take the seasonal difference, so we will take the difference between the last value at the same time that season, at lag 12. We then perform a "first difference" on top of this that should make the graph relatively stationary. Upon visual inspection the data does appear to be stationary.

From this we can perform a unit root test. The unit root test is based under the null hypothesis that the data is stationary, and we shall try and find evidence against that.

```{r}
myseries %>%
  mutate(diff_Turnover= difference(difference(box_cox(Turnover, lambda),12),1)) %>% features(diff_Turnover, unitroot_kpss)

```
Above we can see that there is a small test statistic of 0.0163, this implies that the p value os greater than 0.1, hence not rejecting the null hypothesis that the data is stationary.


## Methodology for creating a shortlist of models.

```{r}
myseries %>% gg_tsdisplay(difference(difference(box_cox(Turnover, lambda),12),1), plot_type='partial', lag=36)
```

We can use the above graphic to try and develop a possible ARIMA model.
at around lag 2 there is a spike, suggesting a MA(2) component. The spike at lag 12 can be attributed of the seasonality, hence we will use the MA(1) component

It is for this reason we obviously will use first the ARIMA(0,1,2)(0,1,1)12

The PACF will now be used to assist with the selection of the None seasonal component. 

ARIMA(1,0,1)(0,1,1)12
Will also be used at it the first MA(2) component is close to 1.
Will also be used, this also has the same difference. We will now add these models. We do not include the differencing part as this will be done by the ARIMA model in the input

.

We will do 2 different ETS models, We can expect that the model that has multiplicative seasonality would perform better due to the hetero scedasticity in the data. 

The multiplicative nature of the seasonal component should ensure that we do not need to use the box-cox transformation.

We will apply the box cox however to the arima model as this will be required. The selection of the double differencing within the model means that transformation will be taken care of itself without requiring the transformation itself.

For THE ARIMA model selection we will obviously use ARIMA models that contain seasonality.

We will also apply the box cox transformation to remove the log from the data.


Testing training split.
Here what we are going to need to do is split the data into a training and testing set.
We are going to take the last 2 years of data as the testing set and the remainder as the training set


```{r}
test_retail <- myseries %>%
  filter(year(Month) >= 2017)

train_retail <- myseries %>%
  filter(year(Month) <= 2016)

fit <- train_retail %>%
  model(
    additive = ETS(Turnover ~ error("A") + trend("A") +
                                                season("A")),
    multiplicative = ETS(Turnover ~ error("M") + trend("A",phi = 0.9) +season("M")), 
     no_season = ETS(Turnover ~ error("A") + trend("A",phi = 0.9) +season("N")),
    `ARIMA012011` = ARIMA(box_cox(Turnover, lambda) ~ pdq(0,1,2) + PDQ(0,1,1)),
    `ARIMA101011` = ARIMA(box_cox(Turnover, lambda) ~ pdq(1,0,1)+PDQ(0,1,1)),
    `ARIMA210011` = ARIMA(box_cox(Turnover, lambda) ~ pdq(2,1,0) + PDQ(0,1,1)),
    
  )
fc <- fit %>% forecast(h = "2 years")##here we are forcasting for the next 5 years.


##Here we can plot the models

fc %>%
  autoplot(
    myseries%>% filter(year(Month)>=2005),
    level = NULL
  )  + labs(y = "Turnover ($m)", title = "All Models")

accuracy(fc, test_retail)##This is how we produce the   
```
Above is the test training split results for all the different models. From this the model that that has the lowest RMSE is the Arima (101)(011).

We will again use the RMSE metric to determine what the best ETS model is. From this we can see that the ETS multiplicative model performs the best, potentially due to the clear increasing trend in the seasonal variation.
```{r}
fit %>%  glance()
```

Above is a glance at the AIC. From this we determine that the model that is the best is actually the ARIMA(210)(011). The best models from the ETS model is the multiplicative.

From this we will select the two models.

The multiplicative model clearly performed the best for both of the tests for AIC and a testing training split.

To select the ARIMA, we will choose the ARIMA(101)(011).
This is because it performs significantly better in the RMSE that the other ARIMA models were as in the AIC it is only slightly behind the ARIMA(210)(011) model.

## Residual Diagnostics


We will first do the residual daignostics for the ETS model.
```{r}

best_fit_ETS <- train_retail %>%
  model(ETS(Turnover ~ error("M") + trend("A",phi = 0.9) +season("M")))

best_fit_ARIMA <- train_retail %>%
  model(ARIMA(box_cox(Turnover, lambda) ~ pdq(1,0,1)+PDQ(0,1,1)))

augment(best_fit_ETS) %>% features(.innov, ljung_box, dof = 2, lag = 10)#here is the code to do the ljung box test
best_fit_ETS %>% gg_tsresiduals()##here is the code to get the ACF and PCF plot

#WHat i need to do now is figure out how to use the Jung box test as well as figure out how to use the ACF plots.
```

From Visual inspection the residuals do appear to be white noise.
For the lb_value, we actually get a relatively small p value. This could be evidence against the null that the graph is not distinguishable from white noise series. Meaning that there could still be some form of trend that has not been taken into account for yet.

The residual histogram does appear to be centered around 0. However if we look at the ACF plot, there are a number of spikes that leave the blue lines. This suggests there still could be some variation that is not fully explaining the model.


we will next do some residual diagnostics for the best ARIMA model
```{r}
augment(best_fit_ARIMA) %>% features(.innov, ljung_box, dof = 2, lag = 10)#here is the code to do the ljung box test
best_fit_ARIMA %>% gg_tsresiduals()
```







Firstly the residuals on visual inspection to appear to be white noise.
The results here prove better than that of the ETS model. From the ljung box test we get the p value of 0.376. Because this is a relatively large p value, we can conclude the residuals are not distinguishable from white noise. The acf plot does show some significant spikes which maybe could still indicate that there is some variation that is not being taken into account of, but there are less of these examples that cross the bars than that of the ETS model.
Again the residuals to seem to center around 0 which is a good indicator there is no further variation to be taken into account for by the model.


## Preferred model
We will now determine which model we think is best with reference to the test set.
```{r}
best_fit_both <- train_retail %>%
  model(multiplicative = ETS(Turnover ~ error("M") + trend("A",phi = 0.9)+ season("M")),
        `ARIMA101011` = ARIMA(box_cox(Turnover, lambda) ~ pdq(1,0,1)+PDQ(0,1,1)))


fc <- best_fit_both %>% forecast(h = "2 years")
fc %>%
  autoplot(
    myseries%>% filter(year(Month)>=2005),
    level = NULL
  ) + labs(
    y = "Turnover $ (M)",
    title = "Forecasts for Turnover"
  ) +
  guides(colour = guide_legend(title = "Forecast"))

accuracy(fc, test_retail)
```


Visually it would appear the the multiplicative ETS model captures the variation better than that of the ARIMA model. Again this can be seen in the RMSE scores in that the ETS multiplicative model has the best root mean square errors.

## THe Full data set.


First we will do 4 years that includes the prediction interval

ETS:
```{r}
best_fit_both <- myseries %>%
  model(multiplicative = ETS(Turnover ~ error("M") + trend("A",phi = 0.9)+ season("M")),
        `ARIMA101011` = ARIMA(box_cox(Turnover, lambda) ~ pdq(1,0,1)+PDQ(0,1,1)))


fc <- best_fit_both %>% forecast(h = "2 years")

fc %>% filter(`.model` == "multiplicative") %>%
  autoplot(
    myseries%>% filter(year(Month)>=2005)) + labs(y = "Turnover $ (M)", title = "Multiplicative 2 years")
```


We will now do the forecasting with the prediction interval for the arima model.
```{r}
fc %>% filter(`.model` == "ARIMA101011") %>%
  autoplot(
    myseries%>% filter(year(Month)>=2005)
  ) + labs(y = "Turnover $ (M)", title = "ARIMA 2 years")
```

The prediction interval for the ARIMA model does appear to be tighter than that of the ETS model. 


## Comparison to real data

To perform this we will use the readabs function, download the series than run a loop that will append it to the data.

We will do it for 4 years of data too see how well the model can perform for longer term forecasts.
```{r}

library(readabs)

##if we have this library we cana use this.
real_data <- read_abs_series("A3349881A")

real_data2 <- real_data %>% select(date,value)

##from this it is already in the date format.
real_data2 <- real_data2 %>% 
  filter(year(date) >= 2019) #%>% filter(year(date)<= 2020)

colnames(real_data2) <- c("Month","Turnover")


##we know that we need to add 24 elements 


for (i in c(1:nrow(real_data2))){

  myseries <- append_row(myseries)##we will add a row to the data
  ##if it is in the right order than it should work correctly
  ##by inspection the data is already sorted so it works fine
  myseries[nrow(myseries), 5]<- real_data2$Turnover[i]##add the turn over
  	
  myseries[nrow(myseries), 3]<-"A3349881A"
  
}
##we know have the full data set and can plot it 
fc_last <- best_fit_both %>% forecast(h = "39 months")

fc_last %>% autoplot(myseries%>% filter(year(Month)>=2009),level = NULL) + labs(y = "Turnover $ (M)", title = "Forecast to date")
```


here we will produce the accuracy score by making a new test set 
```{r}
test_retail_real <- myseries %>%
  filter(year(Month) >= 2019)

accuracy(fc, test_retail_real)
```

We can see here that for both of these forecasts as time increased, the forecasts changed drastically. The first year for both models was very accurate and as time increased both models could not handle the huge jumps in turnover. The RMSE for the ARIMA model was calculated to be 54.57, while the multiplicative had an RMSE of 60.34685, implying for this extrapolation of data the ARIMA performed slightly better. 

## Discussion of benifits and limitations of the data.

It is hard to compare models from different classes as a simple metric like AIC cannot be used as they are calculated differently for both the ARIMA and ETS models. One important aspect to note is the confidence intervals. The confidence intervals for the ETS model are clearly much larger than that of the ARIMA which could be taken into consideration for future modelling. We do notice that both models really struggle to pick up the large increase in turn over seen in 2021 and 2022. Both models do appear to pick up well on the seasonality, and in this regard I would say the two model performances are on par.

The other issue is that for the testing set the ETS model performed better based on the RMSE while for the extrapolation into today data it performs worse than that of the ARIMA model. 

It is hard to overall decide which model to select, as based on the different metrics of AIC, RMSE for the initial testing vs extrapolation to today data, different models come out on top. Instead I would suggest the combination of many different models to predict it. What if we were to take the average of each of the predictions from these two models and see how that performs?




